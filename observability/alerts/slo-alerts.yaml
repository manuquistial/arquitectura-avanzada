# SLO-based Prometheus Alerts
# Based on Google SRE best practices

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: carpeta-ciudadana-slo-alerts
  namespace: carpeta-ciudadana-dev
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: slo-availability
    interval: 30s
    rules:
    # SLO: 99.9% availability (43 minutes downtime/month)
    - alert: SLO_AvailabilityBudgetBurning_Fast
      expr: |
        (
          sum(rate(http_server_requests_total{status_code!~"5.."}[1m]))
          /
          sum(rate(http_server_requests_total[1m]))
        ) < 0.999
      for: 2m
      labels:
        severity: critical
        slo: availability
        window: fast
      annotations:
        summary: "SLO availability budget burning fast"
        description: "Service {{ $labels.service_name }} has {{ $value | humanizePercentage }} availability (SLO: 99.9%)"
        dashboard: "https://grafana.example.com/d/overview"
        runbook: "https://docs.example.com/runbooks/availability"
    
    - alert: SLO_AvailabilityBudgetBurning_Slow
      expr: |
        (
          sum(rate(http_server_requests_total{status_code!~"5.."}[30m]))
          /
          sum(rate(http_server_requests_total[30m]))
        ) < 0.999
      for: 15m
      labels:
        severity: warning
        slo: availability
        window: slow
      annotations:
        summary: "SLO availability budget burning slowly"
        description: "Service {{ $labels.service_name }} availability trending down: {{ $value | humanizePercentage }}"

  - name: slo-latency
    interval: 30s
    rules:
    # SLO: 95% of requests < 500ms
    - alert: SLO_LatencyBudgetExceeded
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_server_request_duration_bucket[5m])) by (le, service_name)
        ) > 0.5
      for: 5m
      labels:
        severity: warning
        slo: latency
      annotations:
        summary: "SLO latency budget exceeded"
        description: "Service {{ $labels.service_name }} P95 latency is {{ $value }}s (SLO: <500ms)"
        dashboard: "https://grafana.example.com/d/latency"
    
    # SLO: 99% of requests < 2s
    - alert: SLO_LatencyP99Critical
      expr: |
        histogram_quantile(0.99,
          sum(rate(http_server_request_duration_bucket[5m])) by (le, service_name)
        ) > 2
      for: 5m
      labels:
        severity: critical
        slo: latency
      annotations:
        summary: "SLO P99 latency critical"
        description: "Service {{ $labels.service_name }} P99 latency is {{ $value }}s (SLO: <2s)"

  - name: slo-error-budget
    interval: 30s
    rules:
    # Error budget: 0.1% (1 error per 1000 requests)
    - alert: SLO_ErrorBudgetExhausted
      expr: |
        (
          sum(rate(http_server_requests_total{status_code=~"5.."}[1h]))
          /
          sum(rate(http_server_requests_total[1h]))
        ) > 0.001
      for: 10m
      labels:
        severity: critical
        slo: error-rate
      annotations:
        summary: "SLO error budget exhausted"
        description: "Service {{ $labels.service_name }} error rate is {{ $value | humanizePercentage }} (budget: 0.1%)"
        impact: "High error rate affecting user experience"
        action: "Investigate logs and recent deployments"
    
    - alert: SLO_ErrorBudgetBurning
      expr: |
        (
          sum(rate(http_server_requests_total{status_code=~"5.."}[5m]))
          /
          sum(rate(http_server_requests_total[5m]))
        ) > 0.0005
      for: 5m
      labels:
        severity: warning
        slo: error-rate
      annotations:
        summary: "SLO error budget burning"
        description: "Service {{ $labels.service_name }} error rate trending up: {{ $value | humanizePercentage }}"

  - name: service-health
    interval: 30s
    rules:
    - alert: ServiceDown
      expr: up{job=~".*carpeta.*"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Service {{ $labels.job }} is down"
        description: "Service {{ $labels.job }} has been down for more than 1 minute"
        impact: "Service unavailable - immediate action required"
    
    - alert: HighMemoryUsage
      expr: |
        (container_memory_working_set_bytes{namespace="carpeta-ciudadana-dev"}
        / container_spec_memory_limit_bytes{namespace="carpeta-ciudadana-dev"}) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage in {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"
    
    - alert: HighCPUUsage
      expr: |
        rate(container_cpu_usage_seconds_total{namespace="carpeta-ciudadana-dev"}[5m]) > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage in {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} CPU usage is high"
    
    - alert: PodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total{namespace="carpeta-ciudadana-dev"}[15m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in last 15 minutes"
        action: "Check logs: kubectl logs {{ $labels.pod }} -n {{ $labels.namespace }}"

  - name: database
    interval: 30s
    rules:
    - alert: DatabaseConnectionPoolExhausted
      expr: |
        (db_connection_pool_active / db_connection_pool_max) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Database connection pool exhausted"
        description: "Service {{ $labels.service_name }} connection pool usage: {{ $value | humanizePercentage }}"
        action: "Consider increasing pool size or investigate slow queries"
    
    - alert: DatabaseSlowQueries
      expr: |
        rate(db_query_duration_seconds_sum[5m]) / rate(db_query_duration_seconds_count[5m]) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Database queries are slow"
        description: "Average query time: {{ $value }}s"
        action: "Review slow query log and add indexes"

  - name: cache
    interval: 30s
    rules:
    - alert: RedisCacheHitRateLow
      expr: |
        (
          sum(rate(redis_cache_hits_total[5m]))
          /
          (sum(rate(redis_cache_hits_total[5m])) + sum(rate(redis_cache_misses_total[5m])))
        ) < 0.7
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Redis cache hit rate is low"
        description: "Cache hit rate: {{ $value | humanizePercentage }} (expected: >70%)"
        action: "Review cache TTLs and key patterns"
    
    - alert: RedisConnectionsFailing
      expr: redis_connected_clients == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Redis has no connected clients"
        description: "Redis instance may be down or unreachable"

  - name: servicebus
    interval: 30s
    rules:
    - alert: ServiceBusQueueBacklog
      expr: servicebus_queue_messages_active > 1000
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Service Bus queue {{ $labels.queue_name }} has backlog"
        description: "Queue depth: {{ $value }} messages"
        action: "Check KEDA scaling and worker performance"
    
    - alert: ServiceBusDLQMessages
      expr: servicebus_queue_messages_deadletter > 10
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Messages in DLQ for queue {{ $labels.queue_name }}"
        description: "{{ $value }} messages in dead-letter queue"
        impact: "Messages failing to process - data loss risk"
        action: "Investigate DLQ messages and fix consumer logic"

  - name: storage
    interval: 30s
    rules:
    - alert: BlobStorageHighLatency
      expr: |
        histogram_quantile(0.95,
          sum(rate(azure_storage_operation_duration_bucket[5m])) by (le)
        ) > 2
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Azure Blob Storage latency high"
        description: "P95 latency: {{ $value }}s"
        action: "Check storage account performance and throttling"
    
    - alert: BlobStorageErrors
      expr: |
        rate(azure_storage_errors_total[5m]) > 0.01
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Azure Blob Storage errors detected"
        description: "Error rate: {{ $value }} errors/s"

  - name: kubernetes
    interval: 30s
    rules:
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is not ready"
        description: "Node has been not ready for more than 5 minutes"
        action: "Check node status: kubectl describe node {{ $labels.node }}"
    
    - alert: PodNotReady
      expr: |
        sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} not ready"
        description: "Pod in {{ $labels.phase }} phase for >10 minutes"
    
    - alert: HPAMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{namespace="carpeta-ciudadana-dev"}
        ==
        kube_horizontalpodautoscaler_spec_max_replicas{namespace="carpeta-ciudadana-dev"}
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "HPA {{ $labels.horizontalpodautoscaler }} maxed out"
        description: "HPA at maximum replicas - consider increasing max"
        action: "Review HPA limits and node capacity"
    
    - alert: PDBViolation
      expr: |
        kube_poddisruptionbudget_status_pod_disruptions_allowed == 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "PDB {{ $labels.poddisruptionbudget }} blocking disruptions"
        description: "No disruptions allowed - maintenance may be blocked"

  - name: security
    interval: 30s
    rules:
    - alert: TooManyFailedLogins
      expr: |
        rate(http_server_requests_total{http_route="/api/auth/login",status_code="401"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        security: auth
      annotations:
        summary: "High rate of failed login attempts"
        description: "{{ $value }} failed logins/s"
        impact: "Possible brute force attack"
        action: "Review auth logs and consider rate limiting"
    
    - alert: SuspiciousAPIUsage
      expr: |
        rate(http_server_requests_total[1m]) > 100
      for: 2m
      labels:
        severity: warning
        security: api
      annotations:
        summary: "Unusual API request rate"
        description: "{{ $value }} req/s from {{ $labels.client_ip }}"
        action: "Review request patterns and check for DoS"

