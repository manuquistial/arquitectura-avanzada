name: CI/CD Pipeline

on:
  push:
    branches: [master, develop]
  pull_request:
    branches: [master, develop]

# Permisos necesarios para federated credentials y security scanning
permissions:
  id-token: write
  contents: read
  security-events: write
  actions: read

env:
  AZURE_REGION: northcentralus
  AKS_CLUSTER_NAME: carpeta-ciudadana-dev
  RESOURCE_GROUP: carpeta-ciudadana-dev-rg
  TERRAFORM_VERSION: 1.6.0

jobs:
  # Lint and Test Frontend
  frontend-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js 22
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: apps/frontend/package-lock.json
      
      - name: Cache Node modules
        uses: actions/cache@v3
        with:
          path: apps/frontend/node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('apps/frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-
      
      - name: Install dependencies
        working-directory: apps/frontend
        run: npm ci
      
      - name: Lint
        working-directory: apps/frontend
        run: npm run lint
      
      - name: Type check
        working-directory: apps/frontend
        run: npx tsc --noEmit
      
      - name: Run tests
        working-directory: apps/frontend
        run: npm run test:unit
        continue-on-error: true

  # Lint and Test Backend Services
  # Unit tests are handled by test.yml workflow
  # Call test.yml workflow
  backend-test:
    uses: ./.github/workflows/test.yml

  # Deploy Infrastructure with Terraform
  infra-apply:
    name: Deploy Azure Infrastructure
    runs-on: ubuntu-latest
    needs: [frontend-test, backend-test]
    if: github.ref == 'refs/heads/master'
    
    outputs:
      resource_group: ${{ steps.terraform-output.outputs.resource_group }}
      aks_cluster_name: ${{ steps.terraform-output.outputs.aks_cluster_name }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Azure Login (Federated Credentials)
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
      
      - name: Create dummy kubeconfig for Helm/K8s providers
        run: |
          echo "ðŸ”§ Creating dummy kubeconfig for provider initialization..."
          mkdir -p ~/.kube
          cat > ~/.kube/config << 'EOF'
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: https://localhost:8443
              insecure-skip-tls-verify: true
            name: dummy-cluster
          contexts:
          - context:
              cluster: dummy-cluster
              user: dummy-user
            name: dummy-context
          current-context: dummy-context
          users:
          - name: dummy-user
            user:
              token: dummy-token
          EOF
          echo "âœ… Dummy kubeconfig created (will be replaced after AKS creation)"
      
      - name: Setup Terraform Backend
        run: |
          echo "ðŸ”§ Setting up Terraform backend (Azure Storage)..."
          
          RESOURCE_GROUP="terraform-state-rg"
          STORAGE_ACCOUNT="tfstatecarpeta"
          CONTAINER_NAME="tfstate"
          LOCATION="northcentralus"
          
          # Verificar/Crear Resource Group
          if ! az group show --name "$RESOURCE_GROUP" &>/dev/null; then
            echo "ðŸ“¦ Creating Resource Group for Terraform state..."
            if az group create \
              --name "$RESOURCE_GROUP" \
              --location "$LOCATION" \
              --tags Purpose="Terraform State Storage" Environment="Shared"; then
              echo "âœ… Resource Group created"
            else
              echo "âŒ ERROR: Failed to create Resource Group"
              echo "âš ï¸  El Service Principal necesita permisos de Contributor o superior"
              echo "âš ï¸  Por favor, crea manualmente el Resource Group:"
              echo "   az group create --name $RESOURCE_GROUP --location $LOCATION"
              exit 1
            fi
          else
            echo "âœ… Resource Group already exists"
          fi
          
          # Verificar/Crear Storage Account
          if ! az storage account show --name "$STORAGE_ACCOUNT" --resource-group "$RESOURCE_GROUP" &>/dev/null; then
            echo "ðŸ“¦ Creating Storage Account for Terraform state..."
            if az storage account create \
              --name "$STORAGE_ACCOUNT" \
              --resource-group "$RESOURCE_GROUP" \
              --location "$LOCATION" \
              --sku Standard_LRS \
              --encryption-services blob \
              --https-only true \
              --min-tls-version TLS1_2 \
              --allow-blob-public-access false \
              --tags Purpose="Terraform State" Environment="Shared"; then
              echo "âœ… Storage Account created"
            else
              echo "âŒ ERROR: Failed to create Storage Account"
              exit 1
            fi
          else
            echo "âœ… Storage Account already exists"
          fi
          
          # Verificar/Crear Container
          if ! az storage container show \
            --name "$CONTAINER_NAME" \
            --account-name "$STORAGE_ACCOUNT" \
            --auth-mode login &>/dev/null; then
            echo "ðŸ“¦ Creating Storage Container for Terraform state..."
            if az storage container create \
              --name "$CONTAINER_NAME" \
              --account-name "$STORAGE_ACCOUNT" \
              --auth-mode login \
              --public-access off; then
              echo "âœ… Storage Container created"
            else
              echo "âŒ ERROR: Failed to create Storage Container"
              exit 1
            fi
          else
            echo "âœ… Storage Container already exists"
          fi
          
          # Habilitar versionado (para seguridad del state)
          echo "ðŸ” Enabling blob versioning for state protection..."
          az storage account blob-service-properties update \
            --resource-group "$RESOURCE_GROUP" \
            --account-name "$STORAGE_ACCOUNT" \
            --enable-versioning true \
            --enable-delete-retention true \
            --delete-retention-days 30 || echo "âš ï¸  Could not enable versioning (may need additional permissions)"
          
          echo ""
          echo "âœ… Terraform backend ready!"
          echo "   ðŸ“ State location: ${STORAGE_ACCOUNT}/${CONTAINER_NAME}/carpeta-ciudadana.tfstate"
      
      - name: Terraform Init
        working-directory: infra/terraform
        run: terraform init
      
      - name: Create terraform.tfvars
        working-directory: infra/terraform
        run: |
          cat > terraform.tfvars << EOF
          # Azure Configuration
          azure_region = "${{ env.AZURE_REGION }}"
          environment  = "dev"
          
          # AKS Configuration
          aks_node_count = 1
          aks_vm_size    = "Standard_B2s"
          
          # PostgreSQL Configuration
          db_admin_username = "${{ secrets.DB_ADMIN_USERNAME }}"
          db_admin_password = "${{ secrets.DB_ADMIN_PASSWORD }}"
          db_sku_name       = "B_Standard_B1ms"
          db_storage_mb     = 32768
          
          # PostgreSQL Firewall (Security)
          db_enable_public_access  = true
          db_aks_egress_ip         = ""
          db_allow_azure_services  = false
          
          # Service Bus
          servicebus_sku = "Basic"
          
          # OpenSearch
          opensearch_username = "admin"
          opensearch_password = "${{ secrets.OPENSEARCH_PASSWORD }}"
          opensearch_storage_size = "8Gi"
          
          # cert-manager
          letsencrypt_email = "${{ secrets.LETSENCRYPT_EMAIL }}"
          
          # Observability
          observability_namespace = "observability"
          otel_replicas = 2
          prometheus_retention = "15d"
          prometheus_storage_size = "10Gi"
          EOF
      
      - name: Terraform Validate
        working-directory: infra/terraform
        run: terraform validate
      
      - name: Terraform Plan (Infrastructure Only)
        working-directory: infra/terraform
        run: |
          echo "ðŸ“‹ Planning infrastructure resources (Stage 1)..."
          terraform plan \
            -target=azurerm_resource_group.main \
            -target=azurerm_user_assigned_identity.aks_identity \
            -target=module.vnet \
            -target=module.aks \
            -target=module.postgresql \
            -target=module.storage \
            -target=module.servicebus \
            -target=module.keyvault \
            -target=azurerm_role_assignment.aks_cluster_to_storage \
            -target=azurerm_role_assignment.aks_identity_to_storage \
            -out=tfplan-infra
      
      - name: Terraform Apply (Infrastructure Only - Stage 1)
        working-directory: infra/terraform
        run: |
          echo "ðŸš€ Applying infrastructure resources (Stage 1)..."
          terraform apply -auto-approve tfplan-infra
          echo "âœ… Infrastructure created successfully"
      
      - name: Configure kubectl for Terraform Helm provider
        run: |
          echo "ðŸ”§ Configuring kubectl for Helm/Kubernetes providers..."
          az aks get-credentials \
            --resource-group ${{ env.RESOURCE_GROUP }} \
            --name ${{ env.AKS_CLUSTER_NAME }} \
            --overwrite-existing
          
          # Verify connection
          kubectl cluster-info || echo "âš ï¸ Cluster not accessible yet"
          echo "âœ… kubectl configured"
      
      - name: Terraform Plan (Helm Charts - Stage 2)
        working-directory: infra/terraform
        run: |
          echo "ðŸ“‹ Planning Helm chart deployments (Stage 2)..."
          terraform plan -out=tfplan-helm
      
      - name: Terraform Apply (Helm Charts - Stage 2)
        working-directory: infra/terraform
        run: |
          echo "ðŸš€ Deploying Helm charts (Stage 2)..."
          terraform apply -auto-approve tfplan-helm
          echo "âœ… Helm charts deployed successfully"
      
      - name: Export Terraform Outputs
        id: terraform-output
        working-directory: infra/terraform
        run: |
          echo "resource_group=$(terraform output -raw resource_group_name)" >> $GITHUB_OUTPUT
          echo "aks_cluster_name=$(terraform output -raw aks_cluster_name)" >> $GITHUB_OUTPUT
          
          # Export sensitive outputs to files for next jobs
          terraform output -raw resource_group_name > /tmp/resource_group.txt
          terraform output -raw aks_cluster_name > /tmp/aks_cluster_name.txt
          terraform output -raw postgresql_fqdn > /tmp/postgres_host.txt
          terraform output -raw postgresql_admin_username > /tmp/postgres_user.txt
          terraform output -raw servicebus_connection_string > /tmp/servicebus_conn.txt
          terraform output -raw storage_account_name > /tmp/storage_account.txt
      
      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: |
            /tmp/resource_group.txt
            /tmp/aks_cluster_name.txt
            /tmp/postgres_host.txt
            /tmp/postgres_user.txt
            /tmp/servicebus_conn.txt
            /tmp/storage_account.txt
          retention-days: 1

  # Install Platform Components (nginx-ingress only - others managed by Terraform)
  platform-install:
    name: Install Nginx Ingress Controller
    runs-on: ubuntu-latest
    needs: [infra-apply]
    if: github.ref == 'refs/heads/master'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download Terraform Outputs
        uses: actions/download-artifact@v4
        with:
          name: terraform-outputs
          path: /tmp
      
      - name: Azure Login (Federated Credentials)
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          resource-group: ${{ needs.infra-apply.outputs.resource_group }}
          cluster-name: ${{ needs.infra-apply.outputs.aks_cluster_name }}
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'
      
      - name: Add Helm Repositories
        run: |
          echo "Adding Helm repositories..."
          
          # ingress-nginx
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          
          # Update repos
          helm repo update
      
      - name: Install Nginx Ingress Controller
        run: |
          echo "Installing Nginx Ingress Controller..."
          echo "Note: cert-manager, KEDA, OpenSearch, and Observability stack are deployed via Terraform"
          
          helm upgrade --install nginx-ingress ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --version 4.8.3 \
            --set controller.service.type=LoadBalancer \
            --set controller.metrics.enabled=true \
            --set controller.podAnnotations."prometheus\.io/scrape"=true \
            --set controller.podAnnotations."prometheus\.io/port"=10254 \
            --set controller.resources.requests.cpu=100m \
            --set controller.resources.requests.memory=128Mi \
            --wait \
            --timeout 8m
          
          echo "âœ… Nginx Ingress installed"
      
      - name: Create ClusterIssuers for cert-manager
        run: |
          echo "Creating Let's Encrypt ClusterIssuers..."
          echo "Waiting for cert-manager to be ready (deployed via Terraform)..."
          
          # Wait for cert-manager to be ready
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=cert-manager \
            -n cert-manager \
            --timeout=5m || echo "cert-manager pods not ready yet"
          
          cat <<EOF | kubectl apply -f -
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-staging
          spec:
            acme:
              server: https://acme-staging-v02.api.letsencrypt.org/directory
              email: ${{ secrets.LETSENCRYPT_EMAIL }}
              privateKeySecretRef:
                name: letsencrypt-staging-account-key
              solvers:
              - http01:
                  ingress:
                    class: nginx
          ---
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-prod
          spec:
            acme:
              server: https://acme-v02.api.letsencrypt.org/directory
              email: ${{ secrets.LETSENCRYPT_EMAIL }}
              privateKeySecretRef:
                name: letsencrypt-prod-account-key
              solvers:
              - http01:
                  ingress:
                    class: nginx
          EOF
          
          echo "âœ… ClusterIssuers created"
      
      - name: Verify Platform Components
        run: |
          echo ""
          echo "========================================="
          echo "Platform Components Status"
          echo "========================================="
          
          echo ""
          echo "cert-manager (deployed via Terraform):"
          kubectl get pods -n cert-manager || echo "cert-manager not found"
          kubectl get clusterissuers || echo "ClusterIssuers not found"
          
          echo ""
          echo "Ingress Controller (deployed via CI/CD):"
          kubectl get pods -n ingress-nginx
          kubectl get svc -n ingress-nginx
          
          echo ""
          echo "Observability (deployed via Terraform):"
          kubectl get pods -n observability || echo "observability namespace not found"
          
          echo ""
          echo "OpenSearch (deployed via Terraform):"
          kubectl get pods -n search || echo "search namespace not found"
          kubectl get svc -n search || echo "search services not found"
          
          echo ""
          echo "KEDA (deployed via Terraform):"
          kubectl get pods -n keda || echo "keda namespace not found"
          
          echo ""
          echo "CSI Secrets Driver (deployed via Terraform):"
          kubectl get pods -n csi-secrets-driver || echo "csi-secrets-driver namespace not found"
          
          echo ""
          echo "========================================="

  # Bootstrap Configuration (Secrets and ConfigMaps)
  bootstrap-config:
    name: Bootstrap Secrets and ConfigMaps
    runs-on: ubuntu-latest
    needs: [platform-install]
    if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Determine Environment
        id: env
        run: |
          if [ "${{ github.ref }}" = "refs/heads/develop" ]; then
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
            echo "allow_insecure_urls=true" >> $GITHUB_OUTPUT
            echo "cors_origins=*" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/master" ]; then
            echo "namespace=carpeta-ciudadana-prod" >> $GITHUB_OUTPUT
            echo "environment=production" >> $GITHUB_OUTPUT
            echo "allow_insecure_urls=false" >> $GITHUB_OUTPUT
            echo "cors_origins=https://carpeta-ciudadana.example.com" >> $GITHUB_OUTPUT
          else
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
            echo "allow_insecure_urls=true" >> $GITHUB_OUTPUT
            echo "cors_origins=*" >> $GITHUB_OUTPUT
          fi
          
          NAMESPACE=$(cat $GITHUB_OUTPUT | grep namespace | cut -d'=' -f2)
          echo "Using namespace: $NAMESPACE"
      
      - name: Download Terraform Outputs
        uses: actions/download-artifact@v4
        with:
          name: terraform-outputs
          path: /tmp
      
      - name: Azure Login (Federated Credentials)
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          resource-group: ${{ needs.infra-apply.outputs.resource_group }}
          cluster-name: ${{ needs.infra-apply.outputs.aks_cluster_name }}
      
      - name: Read Terraform Outputs
        id: tf-outputs
        run: |
          POSTGRES_HOST=$(cat /tmp/postgres_host.txt)
          POSTGRES_USER=$(cat /tmp/postgres_user.txt)
          SERVICEBUS_CONN=$(cat /tmp/servicebus_conn.txt)
          STORAGE_ACCOUNT=$(cat /tmp/storage_account.txt)
          
          echo "POSTGRES_HOST=$POSTGRES_HOST" >> $GITHUB_ENV
          echo "POSTGRES_USER=$POSTGRES_USER" >> $GITHUB_ENV
          echo "SERVICEBUS_CONN=$SERVICEBUS_CONN" >> $GITHUB_ENV
          echo "STORAGE_ACCOUNT=$STORAGE_ACCOUNT" >> $GITHUB_ENV
      
      - name: Create Database Secrets
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating database secrets in namespace: $NAMESPACE..."
          
          # Create namespace if it doesn't exist
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
          
          # Build DATABASE_URL
          DATABASE_URL="postgresql://${{ secrets.DB_ADMIN_USERNAME }}:${{ secrets.DB_ADMIN_PASSWORD }}@${POSTGRES_HOST}/carpeta_ciudadana?sslmode=require"
          
          kubectl create secret generic db-secrets \
            --namespace $NAMESPACE \
            --from-literal=POSTGRES_HOST="${POSTGRES_HOST}" \
            --from-literal=POSTGRES_USER="${POSTGRES_USER}" \
            --from-literal=POSTGRES_PASSWORD="${{ secrets.DB_ADMIN_PASSWORD }}" \
            --from-literal=POSTGRES_DB="carpeta_ciudadana" \
            --from-literal=DATABASE_URL="${DATABASE_URL}" \
            --from-literal=POSTGRES_URI="${DATABASE_URL}" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… Database secrets created"
      
      - name: Create Service Bus Secrets
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating Service Bus secrets in namespace: $NAMESPACE..."
          
          kubectl create secret generic sb-secrets \
            --namespace $NAMESPACE \
            --from-literal=SERVICEBUS_CONNECTION_STRING="${SERVICEBUS_CONN}" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… Service Bus secrets created"
      
      - name: Create Redis Secrets
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating Redis secrets in namespace: $NAMESPACE..."
          
          REDIS_PASSWORD="${{ secrets.REDIS_PASSWORD }}"
          if [ -z "$REDIS_PASSWORD" ]; then
            REDIS_PASSWORD=""
            echo "âš ï¸  REDIS_PASSWORD not set, using empty password"
          fi
          
          kubectl create secret generic redis-auth \
            --namespace $NAMESPACE \
            --from-literal=REDIS_PASSWORD="${REDIS_PASSWORD}" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… Redis secrets created"
      
      - name: Create OpenSearch Secrets
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating OpenSearch secrets in namespace: $NAMESPACE..."
          echo "Note: OpenSearch itself is deployed via Terraform"
          
          # Create in app namespace for app pods to connect to OpenSearch
          kubectl create secret generic opensearch-auth \
            --namespace $NAMESPACE \
            --from-literal=OS_USERNAME=admin \
            --from-literal=OS_PASSWORD="${{ secrets.OPENSEARCH_PASSWORD }}" \
            --from-literal=OPENSEARCH_URL="http://opensearch-cluster-master.search.svc.cluster.local:9200" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… OpenSearch secrets created in app namespace"
      
      - name: Create Azure Storage Secrets
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating Azure Storage secrets in namespace: $NAMESPACE..."
          
          kubectl create secret generic azure-storage \
            --namespace $NAMESPACE \
            --from-literal=AZURE_STORAGE_ACCOUNT_NAME="${STORAGE_ACCOUNT}" \
            --from-literal=AZURE_STORAGE_USE_MANAGED_IDENTITY="true" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… Azure Storage secrets created"
      
      - name: Create SMTP Secrets (optional)
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Creating SMTP secrets in namespace: $NAMESPACE..."
          
          SMTP_HOST="${{ secrets.SMTP_HOST }}"
          SMTP_USER="${{ secrets.SMTP_USER }}"
          SMTP_PASS="${{ secrets.SMTP_PASSWORD }}"
          SMTP_FROM="${{ secrets.SMTP_FROM }}"
          
          if [ -z "$SMTP_HOST" ]; then
            SMTP_HOST="localhost"
            SMTP_USER=""
            SMTP_PASS=""
            SMTP_FROM="noreply@carpeta-ciudadana.example.com"
            echo "âš ï¸  SMTP credentials not set, using defaults"
          fi
          
          kubectl create secret generic smtp-credentials \
            --namespace $NAMESPACE \
            --from-literal=SMTP_HOST="${SMTP_HOST}" \
            --from-literal=SMTP_PORT="${{ secrets.SMTP_PORT || '587' }}" \
            --from-literal=SMTP_USER="${SMTP_USER}" \
            --from-literal=SMTP_PASSWORD="${SMTP_PASS}" \
            --from-literal=SMTP_FROM="${SMTP_FROM}" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… SMTP secrets created"
      
      - name: Create app-flags ConfigMap
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          ENVIRONMENT="${{ steps.env.outputs.environment }}"
          ALLOW_INSECURE_URLS="${{ steps.env.outputs.allow_insecure_urls }}"
          CORS_ORIGINS="${{ steps.env.outputs.cors_origins }}"
          
          echo "Creating app-flags ConfigMap in namespace: $NAMESPACE..."
          echo "Environment: $ENVIRONMENT"
          echo "Allow Insecure URLs: $ALLOW_INSECURE_URLS"
          echo "CORS Origins: $CORS_ORIGINS"
          
          kubectl create configmap app-flags \
            --namespace $NAMESPACE \
            --from-literal=ENVIRONMENT="${ENVIRONMENT}" \
            --from-literal=HUB_BASE_URL="https://govcarpeta-apis-4905ff3c005b.herokuapp.com" \
            --from-literal=ALLOW_INSECURE_OPERATOR_URLS="${ALLOW_INSECURE_URLS}" \
            --from-literal=RATE_LIMIT_TO_HUB="10" \
            --from-literal=SAS_TTL_MINUTES="15" \
            --from-literal=CB_WINDOW="60s" \
            --from-literal=CB_THRESHOLD="5" \
            --from-literal=CB_COOLDOWN="120s" \
            --from-literal=CORS_ALLOWED_ORIGINS="${CORS_ORIGINS}" \
            --from-literal=OTEL_EXPORTER_OTLP_ENDPOINT="http://otel-collector-opentelemetry-collector.observability.svc.cluster.local:4317" \
            --from-literal=OTEL_EXPORTER_OTLP_INSECURE="true" \
            --from-literal=OTEL_SERVICE_NAME="carpeta-ciudadana" \
            --from-literal=LOG_LEVEL="INFO" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… app-flags ConfigMap created"
      
      - name: Create CORS Origins ConfigMap
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          CORS_ORIGINS="${{ steps.env.outputs.cors_origins }}"
          
          echo "Creating cors-origins ConfigMap in namespace: $NAMESPACE..."
          
          kubectl create configmap cors-origins \
            --namespace $NAMESPACE \
            --from-literal=ALLOWED_ORIGINS="${CORS_ORIGINS}" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "âœ… cors-origins ConfigMap created"
      
      - name: Verify Secrets and ConfigMaps
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo ""
          echo "========================================="
          echo "Secrets and ConfigMaps Status"
          echo "Namespace: $NAMESPACE"
          echo "========================================="
          
          echo ""
          echo "Secrets:"
          kubectl get secrets -n $NAMESPACE | grep -E "db-secrets|sb-secrets|redis-auth|opensearch-auth|azure-storage|smtp-credentials" || echo "No secrets found"
          
          echo ""
          echo "ConfigMaps:"
          kubectl get configmaps -n $NAMESPACE | grep -E "app-flags|cors-origins" || echo "No configmaps found"
          
          echo ""
          echo "Detailed app-flags ConfigMap:"
          kubectl describe configmap app-flags -n $NAMESPACE || echo "app-flags not found"
          
          echo ""
          echo "========================================="

  # Run Database Migrations
  run-migrations:
    name: Run Database Migrations
    runs-on: ubuntu-latest
    needs: [bootstrap-config]
    if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Determine Environment
        id: env
        run: |
          if [ "${{ github.ref }}" = "refs/heads/develop" ]; then
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/master" ]; then
            echo "namespace=carpeta-ciudadana-prod" >> $GITHUB_OUTPUT
          else
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
          fi
          
          NAMESPACE=$(cat $GITHUB_OUTPUT | grep namespace | cut -d'=' -f2)
          echo "Using namespace: $NAMESPACE"
      
      - name: Azure Login (Federated Credentials)
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          resource-group: ${{ needs.infra-apply.outputs.resource_group }}
          cluster-name: ${{ needs.infra-apply.outputs.aks_cluster_name }}
      
      - name: Create Migration Jobs
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          IMAGE_TAG="${{ github.sha }}"
          IMAGE_REGISTRY="${{ secrets.DOCKERHUB_USERNAME }}"
          
          echo "Creating migration jobs for database schemas in namespace: $NAMESPACE..."
          
          # Create namespace if it doesn't exist
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
          
          # Create migration job for citizen service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-citizen-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: citizen
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: citizen
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-citizen:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running citizen database migrations..."
                      cd /app
                      alembic upgrade head
                      echo "âœ… Citizen migrations completed"
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for metadata service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-metadata-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: metadata
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: metadata
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-metadata:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running metadata database migrations..."
                      cd /app
                      alembic upgrade head
                      echo "âœ… Metadata migrations completed"
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  - secretRef:
                      name: opensearch-auth
                      optional: true
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for transfer service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-transfer-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: transfer
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: transfer
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-transfer:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running transfer database migrations..."
                      cd /app
                      alembic upgrade head
                      echo "âœ… Transfer migrations completed"
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  - secretRef:
                      name: sb-secrets
                      optional: true
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for read-models service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-read-models-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: read-models
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: read-models
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-read-models:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running read-models database migrations..."
                      cd /app
                      alembic upgrade head
                      echo "âœ… Read-models migrations completed"
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  - secretRef:
                      name: opensearch-auth
                      optional: true
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for ingestion service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-ingestion-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: ingestion
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: ingestion
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-ingestion:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running ingestion database migrations..."
                      cd /app
                      if [ -f "alembic.ini" ]; then
                        alembic upgrade head
                        echo "âœ… Ingestion migrations completed"
                      else
                        echo "âš ï¸  No alembic.ini found, skipping migrations"
                      fi
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  - secretRef:
                      name: azure-storage
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for signature service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-signature-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: signature
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: signature
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-signature:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running signature database migrations..."
                      cd /app
                      if [ -f "alembic.ini" ]; then
                        alembic upgrade head
                        echo "âœ… Signature migrations completed"
                      else
                        echo "âš ï¸  No alembic.ini found, skipping migrations"
                      fi
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for sharing service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-sharing-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: sharing
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: sharing
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-sharing:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running sharing database migrations..."
                      cd /app
                      if [ -f "alembic.ini" ]; then
                        alembic upgrade head
                        echo "âœ… Sharing migrations completed"
                      else
                        echo "âš ï¸  No alembic.ini found, skipping migrations"
                      fi
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          # Create migration job for notification service
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: migrate-notification-${{ github.run_number }}
            namespace: $NAMESPACE
            labels:
              app: carpeta-ciudadana
              service: notification
              migration: "true"
          spec:
            ttlSecondsAfterFinished: 300
            backoffLimit: 3
            template:
              metadata:
                labels:
                  app: carpeta-ciudadana
                  service: notification
                  migration: "true"
              spec:
                restartPolicy: Never
                containers:
                - name: migrate
                  image: ${IMAGE_REGISTRY}/carpeta-notification:${IMAGE_TAG}
                  command: ["sh", "-c"]
                  args:
                    - |
                      echo "Running notification database migrations..."
                      cd /app
                      if [ -f "alembic.ini" ]; then
                        alembic upgrade head
                        echo "âœ… Notification migrations completed"
                      else
                        echo "âš ï¸  No alembic.ini found, skipping migrations"
                      fi
                  envFrom:
                  - configMapRef:
                      name: app-flags
                  - secretRef:
                      name: db-secrets
                  resources:
                    requests:
                      cpu: 50m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
          EOF
          
          echo "âœ… All migration jobs created"
      
      - name: Wait for Migrations to Complete
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          RUN_NUMBER="${{ github.run_number }}"
          
          echo "Waiting for migration jobs to complete..."
          echo ""
          
          # Wait for citizen migration
          echo "Waiting for citizen migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-citizen-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Citizen migration completed"
          
          # Wait for metadata migration
          echo "Waiting for metadata migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-metadata-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Metadata migration completed"
          
          # Wait for transfer migration
          echo "Waiting for transfer migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-transfer-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Transfer migration completed"
          
          # Wait for read-models migration
          echo "Waiting for read-models migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-read-models-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Read-models migration completed"
          
          # Wait for ingestion migration
          echo "Waiting for ingestion migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-ingestion-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Ingestion migration completed"
          
          # Wait for signature migration
          echo "Waiting for signature migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-signature-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Signature migration completed"
          
          # Wait for sharing migration
          echo "Waiting for sharing migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-sharing-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Sharing migration completed"
          
          # Wait for notification migration
          echo "Waiting for notification migration..."
          kubectl wait --for=condition=complete \
            --timeout=10m \
            job/migrate-notification-${RUN_NUMBER} \
            -n $NAMESPACE
          echo "âœ… Notification migration completed"
          
          echo ""
          echo "========================================="
          echo "All migrations completed successfully!"
          echo "========================================="
      
      - name: Show Migration Logs
        if: always()
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          RUN_NUMBER="${{ github.run_number }}"
          
          echo ""
          echo "========================================="
          echo "Migration Logs"
          echo "========================================="
          
          echo ""
          echo "--- Citizen Migration ---"
          kubectl logs job/migrate-citizen-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Metadata Migration ---"
          kubectl logs job/migrate-metadata-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Transfer Migration ---"
          kubectl logs job/migrate-transfer-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Read-models Migration ---"
          kubectl logs job/migrate-read-models-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Ingestion Migration ---"
          kubectl logs job/migrate-ingestion-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Signature Migration ---"
          kubectl logs job/migrate-signature-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Sharing Migration ---"
          kubectl logs job/migrate-sharing-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "--- Notification Migration ---"
          kubectl logs job/migrate-notification-${RUN_NUMBER} -n $NAMESPACE || echo "No logs available"
          
          echo ""
          echo "========================================="
      
      - name: Cleanup Failed Migrations
        if: failure()
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          RUN_NUMBER="${{ github.run_number }}"
          
          echo "Cleaning up failed migration jobs..."
          kubectl delete job migrate-citizen-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-metadata-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-transfer-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-read-models-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-ingestion-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-signature-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-sharing-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true
          kubectl delete job migrate-notification-${RUN_NUMBER} -n $NAMESPACE --ignore-not-found=true

  # Build and Push Docker Images to Docker Hub (Gratis)
  build-and-push:
    name: Build and Push Images
    runs-on: ubuntu-latest
    needs: [bootstrap-config]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop')
    strategy:
      matrix:
        service:
          - frontend
          - gateway
          - citizen
          - ingestion
          - metadata
          - transfer
          - mintic_client
          - signature
          - sharing
          - notification
          - read_models
          - auth
          - transfer_worker
    steps:
      - uses: actions/checkout@v4
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ${{ matrix.service == 'frontend' && 'apps/frontend/Dockerfile' || format('services/{0}/Dockerfile', matrix.service) }}
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:${{ github.sha }}

  # Deploy to AKS usando Federated Credentials
  deploy:
    name: Deploy to AKS
    needs: [run-migrations, build-and-push]
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop')
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Determine Environment
        id: env
        run: |
          if [ "${{ github.ref }}" = "refs/heads/develop" ]; then
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
            echo "values_file=values-dev.yaml" >> $GITHUB_OUTPUT
            echo "allow_insecure=true" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/master" ]; then
            echo "namespace=carpeta-ciudadana-prod" >> $GITHUB_OUTPUT
            echo "values_file=values-prod.yaml" >> $GITHUB_OUTPUT
            echo "allow_insecure=false" >> $GITHUB_OUTPUT
            echo "environment=production" >> $GITHUB_OUTPUT
          else
            echo "namespace=carpeta-ciudadana-dev" >> $GITHUB_OUTPUT
            echo "values_file=values-dev.yaml" >> $GITHUB_OUTPUT
            echo "allow_insecure=true" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
          fi
          
          NAMESPACE=$(cat $GITHUB_OUTPUT | grep namespace | cut -d'=' -f2)
          echo "Deploying to namespace: $NAMESPACE"
      
      # Azure Login con Federated Credentials (sin Service Principal!)
      - name: Azure Login (Federated)
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Set AKS context
        uses: azure/aks-set-context@v3
        with:
          resource-group: ${{ env.RESOURCE_GROUP }}
          cluster-name: ${{ env.AKS_CLUSTER_NAME }}
      
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'
      
      - name: Deploy with Helm
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          VALUES_FILE="${{ steps.env.outputs.values_file }}"
          ALLOW_INSECURE="${{ steps.env.outputs.allow_insecure }}"
          ENVIRONMENT="${{ steps.env.outputs.environment }}"
          
          echo "========================================="
          echo "Deployment Configuration"
          echo "========================================="
          echo "Namespace: $NAMESPACE"
          echo "Values File: $VALUES_FILE"
          echo "Environment: $ENVIRONMENT"
          echo "Allow Insecure URLs: $ALLOW_INSECURE"
          echo "========================================="
          
          # Create namespace if it doesn't exist
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
          
          helm upgrade --install carpeta-ciudadana \
            deploy/helm/carpeta-ciudadana \
            --values deploy/helm/carpeta-ciudadana/$VALUES_FILE \
            --namespace $NAMESPACE \
            --create-namespace \
            --set global.imageRegistry=${{ secrets.DOCKERHUB_USERNAME }} \
            --set frontend.image.tag=${{ github.sha }} \
            --set gateway.image.tag=${{ github.sha }} \
            --set citizen.image.tag=${{ github.sha }} \
            --set ingestion.image.tag=${{ github.sha }} \
            --set metadata.image.tag=${{ github.sha }} \
            --set transfer.image.tag=${{ github.sha }} \
            --set minticClient.image.tag=${{ github.sha }} \
            --set signature.image.tag=${{ github.sha }} \
            --set sharing.image.tag=${{ github.sha }} \
            --set notification.image.tag=${{ github.sha }} \
            --set readModels.image.tag=${{ github.sha }} \
            --set auth.image.tag=${{ github.sha }} \
            --set transferWorker.image.tag=${{ github.sha }} \
            --set servicebus.connectionString="${{ secrets.SERVICEBUS_CONNECTION_STRING }}" \
            --set redis.password="${{ secrets.REDIS_PASSWORD }}" \
            --set opensearch.username="admin" \
            --set opensearch.password="${{ secrets.OPENSEARCH_PASSWORD }}" \
            --set global.hubBaseUrl="https://govcarpeta-apis-4905ff3c005b.herokuapp.com" \
            --set global.allowInsecureOperatorUrls="$ALLOW_INSECURE" \
            --set global.sasTtlMinutes="15" \
            --set global.rateLimitToHub="10" \
            --wait \
            --timeout 20m
          
          echo "âœ… Helm deployment complete"
      
      - name: Verify Pods are Running
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo ""
          echo "========================================="
          echo "Verifying Pod Status"
          echo "========================================="
          
          # Wait a bit for pods to start
          echo "Waiting 30 seconds for pods to initialize..."
          sleep 30
          
          echo ""
          echo "Getting all pods in namespace $NAMESPACE:"
          kubectl get pods -n $NAMESPACE -o wide
          
          echo ""
          echo "Checking for Running pods..."
          RUNNING_PODS=$(kubectl get pods -n $NAMESPACE --field-selector=status.phase=Running --no-headers | wc -l)
          TOTAL_PODS=$(kubectl get pods -n $NAMESPACE --no-headers | wc -l)
          
          echo "Running pods: $RUNNING_PODS / $TOTAL_PODS"
          
          echo ""
          echo "Pods not in Running state:"
          kubectl get pods -n $NAMESPACE --field-selector=status.phase!=Running --no-headers || echo "All pods are Running! âœ…"
          
          echo ""
          echo "Pod status summary:"
          kubectl get pods -n $NAMESPACE --no-headers | awk '{print $3}' | sort | uniq -c
          
          # Check if we have at least some running pods
          if [ "$RUNNING_PODS" -eq 0 ]; then
            echo ""
            echo "âŒ ERROR: No pods are in Running state!"
            echo "Showing pod details for debugging:"
            kubectl describe pods -n $NAMESPACE
            exit 1
          fi
          
          # Show pods that are not running
          NOT_RUNNING=$(kubectl get pods -n $NAMESPACE --field-selector=status.phase!=Running --no-headers | wc -l)
          if [ "$NOT_RUNNING" -gt 0 ]; then
            echo ""
            echo "âš ï¸  WARNING: $NOT_RUNNING pods are not in Running state"
            echo "Details:"
            kubectl get pods -n $NAMESPACE --field-selector=status.phase!=Running
            
            # Show logs for failed pods
            echo ""
            echo "Logs for non-running pods:"
            kubectl get pods -n $NAMESPACE --field-selector=status.phase!=Running --no-headers | awk '{print $1}' | while read pod; do
              echo ""
              echo "--- Logs for $pod ---"
              kubectl logs $pod -n $NAMESPACE --tail=50 || echo "Could not retrieve logs for $pod"
            done
          else
            echo ""
            echo "âœ… All pods are in Running state!"
          fi
          
          echo ""
          echo "========================================="
      
      - name: Wait for Pods to be Ready
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo "Waiting for all deployments to be ready..."
          
          # Wait for each deployment
          for deployment in $(kubectl get deployments -n $NAMESPACE -o jsonpath='{.items[*].metadata.name}'); do
            echo "Waiting for deployment/$deployment..."
            kubectl rollout status deployment/$deployment -n $NAMESPACE --timeout=5m || {
              echo "âŒ Deployment $deployment failed to become ready"
              kubectl describe deployment/$deployment -n $NAMESPACE
              kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=$deployment
            }
          done
          
          echo ""
          echo "âœ… All deployments are ready!"
      
      - name: Run Health Checks
        run: |
          NAMESPACE="${{ steps.env.outputs.namespace }}"
          
          echo ""
          echo "========================================="
          echo "Running Health Checks"
          echo "========================================="
          
          # Get gateway pod
          GATEWAY_POD=$(kubectl get pods -n $NAMESPACE -l app=carpeta-ciudadana,service=gateway -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$GATEWAY_POD" ]; then
            echo "Testing gateway health endpoint..."
            kubectl exec -n $NAMESPACE $GATEWAY_POD -- curl -f http://localhost:8000/health || {
              echo "âš ï¸  Gateway health check failed"
            }
            
            echo ""
            echo "Testing gateway ready endpoint..."
            kubectl exec -n $NAMESPACE $GATEWAY_POD -- curl -f http://localhost:8000/ready || {
              echo "âš ï¸  Gateway readiness check failed"
            }
          else
            echo "âš ï¸  Gateway pod not found, skipping health checks"
          fi
          
          echo ""
          echo "========================================="

  # Security Scan
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: [build-and-push]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop')
    permissions:
      contents: read
      security-events: write
    
    strategy:
      matrix:
        service:
          - frontend
          - gateway
          - citizen
          - ingestion
          - metadata
          - transfer
          - mintic-client
          - signature
          - read-models
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy filesystem scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-fs-results.sarif'
      
      - name: Upload Trivy filesystem results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-fs-results.sarif'
          category: 'filesystem-scan'
        continue-on-error: true
      
      - name: Run Trivy image scanner (latest tag)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:latest'
          format: 'sarif'
          output: 'trivy-image-latest-${{ matrix.service }}.sarif'
          severity: 'CRITICAL,HIGH'
        continue-on-error: true
      
      - name: Upload Trivy image results (latest)
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-image-latest-${{ matrix.service }}.sarif'
          category: 'image-scan-latest-${{ matrix.service }}'
        continue-on-error: true
      
      - name: Run Trivy image scanner (SHA tag)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-image-sha-${{ matrix.service }}.sarif'
          severity: 'CRITICAL,HIGH'
        continue-on-error: true
      
      - name: Upload Trivy image results (SHA)
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-image-sha-${{ matrix.service }}.sarif'
          category: 'image-scan-sha-${{ matrix.service }}'
        continue-on-error: true
      
      - name: Generate Trivy summary report
        if: always()
        run: |
          echo "## ðŸ”’ Security Scan Summary - ${{ matrix.service }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Image:** \`${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Scan image with table output for summary
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:latest || true
          
          echo "### Vulnerabilities (latest tag)" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          trivy image --severity CRITICAL,HIGH --format table \
            ${{ secrets.DOCKERHUB_USERNAME }}/carpeta-${{ matrix.service }}:latest \
            || echo "No vulnerabilities found or scan failed" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY


  # E2E Tests (After images are built and pushed)
  e2e-tests-registry:
    name: E2E Tests (From Registry)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [build-and-push]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/develop')
    
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium]  # Only chromium for speed in CI
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: tests/e2e/package-lock.json
      
      - name: Install dependencies
        working-directory: tests/e2e
        run: npm ci
      
      - name: Install Playwright Browsers
        working-directory: tests/e2e
        run: npx playwright install --with-deps ${{ matrix.browser }}
      
      - name: Pull images from Docker Hub
        run: |
          export DOCKER_USERNAME=${{ secrets.DOCKERHUB_USERNAME }}
          export TAG=latest
          echo "ðŸ” Pulling images from Docker Hub registry..."
          echo "   Registry: $DOCKER_USERNAME/carpeta-*:$TAG"
          docker compose --profile app pull
          echo "âœ… All images pulled successfully from registry"
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          TAG: latest
      
      - name: Start services
        run: |
          export DOCKER_USERNAME=${{ secrets.DOCKERHUB_USERNAME }}
          export TAG=latest
          docker compose --profile app up -d
          sleep 30  # Wait for services to initialize
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          TAG: latest
      
      - name: Wait for frontend
        run: npx wait-on http://localhost:3000 --timeout 120000
      
      - name: Show service status
        if: always()
        run: |
          echo "=== Docker Compose Services ==="
          docker compose ps
          echo ""
          echo "=== Frontend Logs (last 50 lines) ==="
          docker compose logs --tail=50 frontend || echo "Frontend logs not available"
      
      - name: Run Playwright E2E tests
        working-directory: tests/e2e
        run: npx playwright test --project=${{ matrix.browser }}
        env:
          BASE_URL: http://localhost:3000
          API_URL: http://localhost:8000
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-results-registry-${{ matrix.browser }}
          path: tests/e2e/test-results/
          retention-days: 7
      
      - name: Upload Playwright report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-report-registry-${{ matrix.browser }}
          path: tests/e2e/playwright-report/
          retention-days: 7
      
      - name: Stop services
        if: always()
        run: |
          docker compose --profile app down -v
          docker compose logs > docker-compose-logs.txt || true
      
      - name: Upload Docker logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: docker-logs-${{ matrix.browser }}
          path: docker-compose-logs.txt
          retention-days: 3
